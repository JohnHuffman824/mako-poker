
14

Automatic Zoom
4 Application To Poker
We now describe how we use counterfactual regret minimization to compute a near equilibrium
solution in the domain of poker. The poker variant we focus on is heads-up limit Texas Holdâ€™em,
as it is used in the AAAI Computer Poker Competition [9]. The game consists of two players
(zero-sum), four rounds of cards being dealt, and four rounds of betting, and has just under 1018
game states [2]. As with all previous work on this domain, we will first abstract the game and
find an equilibrium of the abstracted game. In the terminology of extensive games, we will merge
information sets; in the terminology of poker, we will bucket card sequences. The quality of the
resulting near equilibrium solution depends on the coarseness of the abstraction. In general, the less
abstraction used, the higher the quality of the resulting strategy. Hence, the ability to solve a larger
game means less abstraction is required, translating into a stronger poker playing program.
4.1 Abstraction
The goal of abstraction is to reduce the number of information sets for each player to a tractable
size such that the abstract game can be solved. Early poker abstractions [2, 4] involved limiting
the possible sequences of bets, e.g., only allowing three bets per round, or replacing all first-round
decisions with a fixed policy. More recently, abstractions involving full four round games with the
full four bets per round have proven to be a significant improvement [7, ?]. We also will keep the
full gameâ€™s betting structure and focus abstraction on the dealt cards.
Our abstraction groups together observed card sequences based on a metric called hand strength
squared. Hand strength is the expected probability of winning1 given only the cards a player has
seen. This was used a great deal in previous work on abstraction [2, 4]. Hand strength squared
is the expected square of the hand strength after the last card is revealed, given only the cards a
player has seen. Intuitively, hand strength squared is similar to hand strength but gives a bonus to
card sequences whose eventual hand strength has higher variance. Higher variance is preferred as it
means the player eventually will be more certain about their ultimate chances of winning prior to a
showdown. More importantly, we will show in Section 5 that this metric for abstraction results in
stronger poker strategies.
The final abstraction is generated by partitioning card sequences based on the hand strength squared
metric. First, all round-one card sequences (i.e., all private card holdings) are partitioned into ten
equally sized buckets based upon the metric. Then, all round-two card sequences that shared a
round-one bucket are partitioned into ten equally sized buckets based on the metric now applied at
round two. Thus, a partition of card sequences in round two is a pair of numbers: its bucket in
the previous round and its bucket in the current round given its bucket in the previous round. This
is repeated after reach round, continuing to partition card sequences that agreed on the previous
roundsâ€™ buckets into ten equally sized buckets based on the metric applied in that round. Thus, card
sequences are partitioned into bucket sequences: a bucket from {1, . . . 10} for each round. The
resulting abstract game has approximately 1.65 Ã— 1012 game states, and 5.73 Ã— 107 information
sets. In the full game of poker, there are approximately 9.17 Ã— 1017 game states and 3.19 Ã— 1014
information sets. So although this represents a significant abstraction on the original game it is two
orders of magnitude larger than previously solved abstractions.
4.2 Minimizing Counterfactual Regret
Now that we have specified an abstraction, we can use counterfactual regret minimization to com-
pute an approximate equilibrium for this game. The basic procedure involves having two players
repeatedly play the game using the counterfactual regret minimizing strategy from Equation 8. Af-
ter T repetitions of the game, or simply iterations, we return ( Ì„ÏƒT
1 ,  Ì„ÏƒT
2 ) as the resulting approximate
equilibrium. Repeated play requires storing Rt
i (I, a) for every information set I and action a, and
updating it after each iteration.2
1Where a tie is considered â€œhalf a winâ€
2The bound from Theorem 4 for the basic procedure can actually be made significantly tighter in the specific
case of poker. In the appendix, we show that the bound for poker is actually independent of the size of the card
abstraction.
5
For our experiments, we actually use a variation of this basic procedure, which exploits the fact
that our abstraction has a small number of information sets relative to the number of game states.
Although each information set is crucial, many consist of a hundred or more individual histories.
This fact suggests it may be possible to get a good idea of the correct behavior for an information
set by only sampling a fraction of the associated game states. In particular, for each iteration, we
sample deterministic actions for the chance player. Thus, Ïƒt
c is set to be a deterministic strategy, but
chosen according to the distribution specified by fc. For our abstraction this amounts to choosing
a joint bucket sequence for the two players. Once the joint bucket sequence is specified, there are
only 18,496 reachable states and 6,378 reachable information sets. Since Ï€Ïƒt
âˆ’i(I) is zero for all other
information sets, no updates need to be made for these information sets.3
This sampling variant allows approximately 750 iterations of the algorithm to be completed in a
single second on a single core of a 2.4Ghz Dual Core AMD Opteron 280 processor. In addition, a
straightforward parallelization is possible and was used when noted in the experiments. Since betting
is public information, the flop-onward information sets for a particular preflop betting sequence can
be computed independently. With four processors we were able to complete approximately 1700
iterations in one second. The complete algorithmic details with pseudocode can be found in the
appendix.
5 Experimental Results
Before discussing the results, it is useful to consider how one evaluates the strength of a near equi-
librium poker strategy. One natural method is to measure the strategyâ€™s exploitability, or its per-
formance against its worst-case opponent. In a symmetric, zero-sum game like heads-up poker4, a
perfect equilibrium has zero exploitability, while an -Nash equilibrium has exploitability . A con-
venient measure of exploitability is millibets-per-hand (mb/h), where a millibet is one thousandth
of a small-bet, the fixed magnitude of bets used in the first two rounds of betting. To provide some
intuition for these numbers, a player that always folds will lose 750 mb/h while a player that is 10
mb/h stronger than another would require over one million hands to be 95% certain to have won
overall.
In general, it is intractable to compute a strategyâ€™s exploitability within the full game. For strategies
in a reasonably sized abstraction it is possible to compute their exploitability within their own ab-
stract game. Such a measure is a useful evaluation of the equilibrium computation technique that
was used to generate the strategy. However, it does not imply the technique cannot be exploited by
a strategy outside of its abstraction. It is therefore common to compare the performance of the strat-
egy in the full game against a battery of known strong poker playing programs. Although positive
expected value against an opponent is not transitive, winning against a large and diverse range of
opponents suggests a strong program.
We used the sampled counterfactual regret minimization procedure to find an approximate equilib-
rium for our abstract game as described in the previous section. The algorithm was run for 2 billion
iterations (T = 2 Ã— 109), or less than 14 days of computation when parallelized across four CPUs.
The resulting strategyâ€™s exploitability within its own abstract game is 2.2 mb/h. After only 200 mil-
lion iterations, or less than 2 days of computation, the strategy was already exploitable by less than
13 mb/h. Notice that the algorithm visits only 18,496 game states per iteration. After 200 million
iterations each game state has been visited less than 2.5 times on average, yet the algorithm has
already computed a relatively accurate solution.
5.1 Scaling the Abstraction
In addition to finding an approximate equilibrium for our large abstraction, we also found approx-
imate equilibria for a number of smaller abstractions. These abstractions used fewer buckets per
round to partition the card sequences. In addition to ten buckets, we also solved eight, six, and five
3A regret analysis of this variant in poker is included in the appendix. We show that the quadratic decrease
in the cost per iteration only causes in a linear increase in the required number of iterations. The experimental
results in the next section coincides with this analysis.
4A single hand of poker is not a symmetric game as the order of betting is strategically significant. However
a pair of hands where the betting order is reversed is symmetric.
6
Abs Size Iterations Time Exp
(Ã—109) (Ã—106) (h) (mb/h)
5 6.45 100 33 3.4
6 27.7 200 75 3.1
8 276 750 261 2.7
10 1646 2000 326â€  2.2
â€ : parallel implementation with 4 CPUs
(a)0
5
10
15
20
25
0 2 4 6 8 10 12 14 16 18
Exploitability (mb/h)
Iterations in thousands, divided by the number of information sets
CFR5
CFR8
CFR10
(b)
Figure 1: (a) Number of game states, number of iterations, computation time, and exploitability (in
its own abstract game) of the resulting strategy for different sized abstractions. (b) Convergence
rates for three different sized abstractions. The x-axis shows the number of iterations divided by the
number of information sets in the abstraction.
bucket variants. As these abstractions are smaller, they require fewer iterations to compute a simi-
larly accurate equilibrium. For example, the program computed with the five bucket approximation
(CFR5) is about 250 times smaller with just under 1010 game states. After 100 million iterations,
or 33 hours of computation without any parallelization, the final strategy is exploitable by 3.4 mb/h.
This is approximately the same size of game solved by recent state-of-the-art algorithms [?, 7] with
many days of computation.
Figure 1b shows a graph of the convergence rates for the five, eight, and ten partition abstractions.
The y-axis is exploitability while the x-axis is the number of iterations normalized by the number
of information sets in the particular abstraction being plotted. The rates of convergence almost
exactly coincide showing that, in practice, the number of iterations needed is growing linearly with
the number of information sets. Due to the use of sampled bucket sequences, the time per iteration
is nearly independent of the size of the abstraction. This suggests that, in practice, the overall
computational complexity is only linear in the size of the chosen card abstraction.
5.2 Performance in Full Texas Holdâ€™em
We have noted that the ability to solve larger games means less abstraction is necessary, resulting
in an overall stronger poker playing program. We have played our four near equilibrium bots with
various abstraction sizes against each other and two other known strong programs: PsOpti4 and
S2298. PsOpti4 is a variant of the equilibrium strategy described in [2]. It was the stronger half
of Hyperborean, the AAAI 2006 Computer Poker Competitionâ€™s winning program. It is available
under the name SparBot in the entertainment program Poker Academy, published by BioTools. We
have calculated strategies that exploit it at 175 mb/h. S2298 is the equilibrium strategy described in
[?]. We have calculated strategies that exploit it at 52.5 mb/h. In terms of the size of the abstract
game PsOpti4 is the smallest consisting of a small number of merged three round games. S2298
restricts the number of bets per round to 3 and uses a five bucket per round card abstraction based
on hand-strength, resulting an abstraction slightly smaller than CFR5.
Table 1 shows a cross table with the results of these matches. Strategies from larger abstractions
consistently, and significantly, outperform their smaller counterparts. The larger abstractions also
consistently exploit weaker bots by a larger margin (e.g., CFR10 wins 19mb/h more from S2298
than CFR5).
Finally, we also played CFR8 against the four bots that competed in the bankroll portion of the 2006
AAAI Computer Poker Competition, which are available on the competitionâ€™s benchmark server [9].
The results are shown in Table 2, along with S2298â€™s previously published performance against the
7
PsOpti4 S2298 CFR5 CFR6 CFR8 CFR10 Average
PsOpti4 0 -28 -36 -40 -52 -55 -35
S2298 28 0 -17 -24 -30 -36 -13
CFR5 36 17 0 -5 -13 -20 2
CFR6 40 24 5 0 -9 -14 7
CFR8 52 30 13 9 0 -6 16
CFR10 55 36 20 14 6 0 22
Max 55 36 20 14 6 0
Table 1: Winnings in mb/h for the row player in full Texas Holdâ€™em. Matches with Opti4 used 10
duplicate matches of 10,000 hands each and are significant to 20 mb/h. Other matches used 10
duplicate matches of 500,000 hands each are are significant to 2 mb/h.
Hyperborean BluffBot Monash Teddy Average
S2298 61 113 695 474 336
CFR8 106 170 746 517 385
Table 2: Winnings in mb/h for the row player in full Texas Holdâ€™em.
same bots [?]. The program not only beats all of the bots from the competition but does so by a
larger margin than S2298.
6 Conclusion
We introduced a new regret concept for extensive games called counterfactual regret. We showed
that minimizing counterfactual regret minimizes overall regret and presented a general and poker-
specific algorithm for efficiently minimizing counterfactual regret. We demonstrated the technique
in the domain of poker, showing that the technique can compute an approximate equilibrium for
abstractions with as many as 1012 states, two orders of magnitude larger than previous methods. We
also showed that the resulting poker playing program outperforms other strong programs, including
all of the competitors from the bankroll portion of the 2006 AAAI Computer Poker Competition.
References
[1] D. Koller and N. Megiddo. The complexity of two-person zero-sum games in extensive form. Games and
Economic Behavior, pages 528â€“552, 1992.
[2] D. Billings, N. Burch, A. Davidson, R. Holte, J. Schaeffer, T. Schauenberg, and D. Szafron. Approximat-
ing game-theoretic optimal strategies for full-scale poker. In International Joint Conference on Artificial
Intelligence, pages 661â€“668, 2003.
[3] A. Gilpin and T. Sandholm. Finding equilibria in large sequential games of imperfect information. In ACM
Conference on Electronic Commerce, 2006.
[4] A. Gilpin and T. Sandholm. A competitive texas holdâ€™em poker player via automated abstraction and
real-time equilibrium computation. In National Conference on Artificial Intelligence, 2006.
[5] G. Gordon. No-regret algorithms for online convex programs. In Neural Information Processing Systems
19, 2007.
[6] M. Zinkevich, M. Bowling, and N. Burch. A new algorithm for generating strong strategies in massive
zero-sum games. In Proceedings of the Twenty-Seventh Conference on Artificial Intelligence (AAAI), 2007.
To Appear.
[7] A. Gilpin, S. Hoda, J. Pena, and T. Sandholm. Gradient-based algorithms for finding nash equilibria in
extensive form games. In Proceedings of the Eighteenth International Conference on Game Theory, 2007.
[8] M. Osborne and A. Rubenstein. A Course in Game Theory. The MIT Press, Cambridge, Massachusetts,
1994.
[9] M. Zinkevich and M. Littman. The AAAI computer poker competition. Journal of the International
Computer Games Association, 29, 2006. News item.
8
A Appendix
A.1 Proof of Theorem 3
Define D(I) to be the information sets of player i reachable from I (including I). Define Ïƒ|D(I)â†’Ïƒâ€² to be a
strategy profile equal to Ïƒ except in the information sets in D(I) where it is equal to Ïƒâ€². The full counterfactual
regret is:
RT
i,full(I) = 1
T max
Ïƒâ€²âˆˆÎ£i
TX
t=1
Ï€Ïƒt
âˆ’i(I) `ui(Ïƒt|D(I)â†’Ïƒâ€² , I) âˆ’ ui(Ïƒt, I) Ì (9)
Again, we define RT,+
i,full(I) = max(RT
i,full(I), 0). Moreover, we define succÏƒ
i (Iâ€²|I, a) to be the probability
that Iâ€² is the next information set of player i visited given that the action a was just selected in information
set I, and Ïƒ is the current strategy. If Ïƒ implies that I is unreachable because of an action of player i, that
action is changed to allow I to be reachable. Define Succi(I, a) to be the set of all possible next information
sets of player i visited given that action a âˆˆ A(I) was just selected in information set I. Define Succi(I) =S
aâˆˆA(I) Succi(I, a).
The following lemma describes the relationship between full and immediate counterfactual regret.
Lemma 5 RT
i,full(I) â‰¤ RT
i,imm(I) + P
Iâ€²âˆˆSucci(I) RT,+
i,full(Iâ€²)
Corollary 6 RT,+
i,full(I) â‰¤ RT,+
i,imm(I) + P
Iâ€²âˆˆSucci(I) RT,+
i,full(Iâ€²)
Proof:
RT
i,full(I) = 1
T max
aâˆˆA(I) max
Ïƒâ€²âˆˆÎ£i
TX
t=1
Ï€Ïƒt
âˆ’i(I)
0
@ui(Ïƒt|Iâ†’a, I) âˆ’ ui(Ïƒt, I) + X
Iâ€²âˆˆSucci(I,a)
succÏƒ
i (Iâ€²|I, a) `ui(Ïƒt|D(I)â†’Ïƒâ€² , Iâ€²) âˆ’ ui(Ïƒt, Iâ€²) Ì
1
A (10)
RT
i,full(I) â‰¤ 1
T max
aâˆˆA(I) max
Ïƒâ€²âˆˆÎ£1
TX
t=1
Ï€Ïƒt
âˆ’i(I) `ui(Ïƒt|Iâ†’a, I) âˆ’ ui(Ïƒt, I) Ì
+ 1
T max
aâˆˆA(I) max
Ïƒâ€²âˆˆÎ£1
TX
t=1
Ï€Ïƒt
âˆ’i(I) X
Iâ€²âˆˆSucci(I,a)
succÏƒ
i (Iâ€²|I, a) `ui(Ïƒt|D(I)â†’Ïƒâ€² , Iâ€²) âˆ’ ui(Ïƒt, Iâ€²) Ì (11)
The first part of the expression on the right hand side is the immediate regret. For the second, we know that
Ï€Ïƒt
âˆ’i(I)succÏƒ
i (Iâ€²|I, a) = Ï€Ïƒt
âˆ’i(Iâ€²), and that ui(Ïƒt|D(I)â†’Ïƒâ€² , Iâ€²) = ui(Ïƒt|D(Iâ€²)â†’Ïƒâ€² , Iâ€²).
RT
i,full(I) â‰¤ RT
i,imm(I)
+ 1
T max
aâˆˆA(I) max
Ïƒâ€²âˆˆÎ£i
TX
t=1
X
Iâ€²âˆˆSucci(I,a)
Ï€Ïƒt
âˆ’i(Iâ€²) `ui(Ïƒt|D(Iâ€²)â†’Ïƒâ€² , Iâ€²) âˆ’ ui(Ïƒt, Iâ€²) Ì (12)
RT
i,full(I) â‰¤ RT
i,imm(I)
+ max
aâˆˆA(I)
X
Iâ€²âˆˆSucci(I,a)
1
T max
Ïƒâ€²âˆˆÎ£i
TX
t=1
Ï€Ïƒt
âˆ’i(Iâ€²) `ui(Ïƒt|D(Iâ€²)â†’Ïƒâ€² , Iâ€²) âˆ’ ui(Ïƒt, Iâ€²) Ì (13)
RT
i,full(I) â‰¤ RT
i,imm(I) + max
aâˆˆA(I)
X
Iâ€²âˆˆSucci(I,a)
RT
i,full(Iâ€²) (14)
Because the game is perfect recall, given distinct a, aâ€² âˆˆ A(I), Succi(I, a) and Succi(I, aâ€²) are disjoint.
Therefore:
RT
i,full(I) â‰¤ RT
i,imm(I) + X
Iâ€²âˆˆSucci(I)
RT,+
i,full(Iâ€²) (15)
9
We prove Theorem 3 by using a lemma that can be proven recursively:
Lemma 7 RT,+
i,full(I) â‰¤ P
Iâ€²âˆˆD(I) RT,+
i,imm(Iâ€²).
Proof: We prove this for a particular game recursively on the size of D(I). Observe that if an information
set has no successors, then Corollary 6 proves the result. We use this as a basis step. Also, observe that
D(I) = {I} âˆª S
Iâ€²âˆˆSucci(I) D(Iâ€²), and that if Iâ€² âˆˆ Succi(I), then I /âˆˆ D(Iâ€²), implying |D(Iâ€²)| < |D(I)|.
Thus, by induction we can establish that:
RT,+
i,full(I) â‰¤ RT,+
i,imm(I) + X
Iâ€²âˆˆSucci(I)
X
Iâ€²â€²âˆˆD(Iâ€²)
RT,+
i,imm(Iâ€²â€²) (16)
Because the game is perfect recall, for any distinct Iâ€², Iâ€²â€² âˆˆ Succi(I), D(Iâ€²) and D(Iâ€²â€²) are disjoint. Therefore:
RT,+
i,imm(I) + X
Iâ€²âˆˆSucci(I)
X
Iâ€²â€²âˆˆD(Iâ€²)
RT,+
i,imm(Iâ€²â€²) = X
Iâ€²âˆˆD(I)
RT,+
i,imm(Iâ€²) (17)
The result immediately follows.
Proof (of Theorem 3): If P (âˆ…) = i, then RT
i,full({âˆ…}) = RT
i , and the theorem follows from Lemma 7. If this
is not the case, then we can simply add a new information set at the beginning of the game, where player i only
has one action.
A.2 Regret Matching
Blackwellâ€™s approachability theorem when applied to minimizing regret is known as regret matching. In
general, regret matching can be defined in a domain where there are a fixed set of actions A, a function ut :
A â†’ R, and on each round a distribution over the actions pt is selected.
Define the regret of not playing action a âˆˆ A until time T as:
Rt(a) = 1
T
TX
t=1
ut(a) âˆ’ X
aâˆˆA
pt(a)ut(a) (18)
and define Rt,+(a) = max(Rt(a), 0). To apply regret matching, one chooses the distribution:
pt(a) =
( Rtâˆ’1,+(a)
P
aâ€²âˆˆA Rtâˆ’1,+(aâ€²) if P
aâ€²âˆˆA Rtâˆ’1,+(aâ€²) > 0
1
|A| otherwise (19)
Theorem 8 If |u| = maxtâˆˆ{1...T } maxa,aâ€²âˆˆA(ut(a) âˆ’ ut(aâ€²)), the regret of the regret matching algorithm is
bounded by:
max
aâˆˆA Rt(a) â‰¤ |u|p|A|
âˆšT (20)
Blackwellâ€™s original result [?] focused on the case where an action (or vector) is chosen at random (instead of
a distribution over actions) and gave a probabilistic guarantee. The result above focuses on the distributions
selected, and is more applicable to a scenario where a probability is selected instead of an action.
For a proof, see [5].
A.3 Proof of Theorem 4
Observe that Equation 8 is an implementation of regret matching. Moreover, observe that for all I âˆˆ Ii,
a âˆˆ A(I), Ï€Ïƒt
âˆ’i(I)(ui(Ïƒt|Iâ†’a, I) âˆ’ ui(Ïƒt, I)) â‰¤ âˆ†u,i. Therefore, Theorem 8 states that the counterfactual
regret of that node will be less than âˆ†u,i
p|A(I)|/âˆšT â‰¤ âˆ†u,i|Ai|/âˆšT . Summing over all I âˆˆ Ii yields the
result.
A.4 Poker-Specific Implementation
We need to iterate over all of the information sets reachable given the joint bucket sequence, and compute prob-
abilities and regrets. In order to do this swiftly, we represent the data in each information set in a â€œplayer view
treeâ€: in other words, we never explicitly represent every state in the abstracted game: instead, we represent the
information sets for each player in its own tree, with each node n being one of four types:
10
â€¢ Bucket Nodes: nodes representing where information about the cards is observed. Has a child node
(an opponent or player node) for each different class that could be observed at that point.
â€¢ Opponent Nodes: nodes representing where the opponent takes an action. Has a child node for each
action.
â€¢ Player Nodes: nodes representing where the current player takes an action. Contains the average
regret with respect to each action, the total probability for each action until this point, and a child
node for each action (either an opponent, bucket, or terminal node). There is an implicit information
set associated with this node, which we will write as I(n).
â€¢ Terminal Nodes: nodes where the game ends due to someone folding or a showdown. Given the
probability of a win,loss, and tie, has sufficient information to compute an expected utility for the
hand given that the node was reached.
Each player observes different pieces of information about the game, and therefore travels to a different part of
its tree during the computation. Our algorithm recurses over both trees in a paired fashion. Before we begin,
define uâ€²
i(Ïƒ, I) = Ï€Ïƒ
âˆ’i(I)ui(Ïƒ, I). For each node in the trees, there will be a value ui(Ïƒ, n) which we use in
order to compute the values ui(Ïƒ, I) and ui(Ïƒ, I, a), which is the expected value given information set I is
reached and action a is taken.
Algorithm 1 WALKTREES(r1, r2, b, p1, p2)
Require: A node r1 for an information set tree for player 1.
Require: A node r2 for an information set tree for player 2.
Require: A joint bucket sequence b.
Require: A probability p1 of player 1 playing to reach the node.
Require: A probability p2 of player 2 playing to reach the node.
Ensure: The utility ui(Ïƒ, ri) for player 1 and player 2.
1: if r1 is a player node (meaning r2 is an opponent node) then
2: Compute Ïƒ1(I(r1)) according to Equation 8.
3: for Each action a âˆˆ A(I(r1)) do
4: Find the associated child of c1 of r1 and c2 of r2.
5: Compute u1(Ïƒ, I(r1), a) and u2(Ïƒ, r2, a) from WALKTREES(c1, c2, b, p1 Ã—
Ïƒ1(I(r1))(a), p2).
6: end for
7: Compute u1(Ïƒ, I(r1))) = âˆ‘
aâˆˆA(I(r1)) Ïƒ1(I(r1))(a)u1(Ïƒ, I(r1), a).
8: for Each action a âˆˆ A(I(r1)) do
9: R1(I, a) = 1
T +1 (T R1(I, a) + p2(u1(Ïƒ, I(r1), a) âˆ’ u1(Ïƒ, I(r1))))
10: end for
11: Set u1(Ïƒ, r1) = u1(Ïƒ, I(r1))
12: Compute u2(Ïƒ, r2) = âˆ‘
aâˆˆA(I(r1)) Ïƒ1(I(r1))(a)u2(Ïƒ, r2, a).
13: else if r2 is a player node (meaning r1 is an opponent node) then
14: do (opposite of above)
15: else if r1 is a bucket node then
16: Choose the child c1 of r1 according to the class in b for player 1 on the appropriate round and
the child c2 of r2 similarly.
17: Find u1(Ïƒ, c1) and u2(Ïƒ, c2) from WALKTREES(c1, c2, b, p1, p2).
18: Set u1(Ïƒ, r1) = u1(Ïƒ, c1) and u2(Ïƒ, r2) = u2(Ïƒ, c2).
19: else if r1 is a terminal node then
20: Find u1(Ïƒ, r1) and u2(Ïƒ, r2), the utility of each player if this node is actually reached.
21: end if
A.5 Poker-Specific Analysis
We first analyze the non-sampling algorithm from Section 3, significantly tightening the presented regret bounds
for the specific case of poker games. We then give a regret analysis for the sampling implementation described
in Section 4 and used in the experimental results presented in Section 5.
A.5.1 Non-Sampling Algorithm
In Section 3, we discussed Blackwellâ€™s Approachability Theorem being applied in every information set. The
disadvantage of such an algorithm is that every iteration involves a walk across the entire game tree. The
11
advantage of such an algorithm is that it converges really quickly in terms of iterations. In this analysis, we
focus on poker.
If we can bound the difference in any two counterfactual utilities at every information set, we can achieve a
bound on the overall regret, because Blackwellâ€™s Approachability Theorem gives a guarantee based upon this.
In particular, after T time steps, if the bound for the counterfactual utility at an information set is âˆ†u,1(I), and
there are |A(I)| actions, then the counterfactual regret is bounded by:
RT
1 (I) â‰¤ âˆ†u,1(I)p|A(I)|
âˆšT (21)
By Theorem 3, this means the average overall regret is bounded by:
RT
1 â‰¤ X
IâˆˆI1
âˆ†u,1(I)p|A(I)|
âˆšT (22)
First of all, define âˆ†u,1 to be the overall range of utilities in limit poker (48 small bets/hand). In particular,
given Ï€0(I) (the probability of chance acting to reach a node), âˆ†u,1(I) â‰¤ Ï€0(I)âˆ†u,1. In limit one could be
more precise, because any information set that begins with both players checking on the pre-flop has a tighter
limit on the maximum won or lost, but bounding based on chance nodes is more crucial. In the next step, we
leverage the structure of poker: in particular, the fact that all actions are observable. Define B1 to be the set
of all betting sequences where the first player has to act: in particular, B1 can be considered a partition of the
information sets I1 (such that each B âˆˆ B1 is a set of information sets). Note that, for all B âˆˆ B1:
X
IâˆˆB
Ï€0(I) = 1 (23)
Moreover, observe that we can define A(B) to be the set of actions available at any information set in B.
Applying these concepts to the equation:
RT
1 â‰¤ X
BâˆˆB1
p|A(B)|âˆ†u,1
âˆšT (24)
RT
1 â‰¤ âˆ†u,1
âˆšT
X
BâˆˆB1
p|A(B)| (25)
Thus, increasing the size of the card abstraction does not affect the rate of convergence. This is not as surprising
as one might think: if one imagined n independent algorithms minimizing regret, each with a bound on their
utility of âˆ†u,1, then one would expect that the theoretical bound on the average of the algorithms would closely
resemble the theoretical bound on the average of one particular algorithm. This is very similar to what was
leveraged in this section. However, the number of information sets does have an affect on the cost of an
iteration: each game state in the abstraction must be traversed in every iteration. This is the primary motivation
for WALKTREES.
A.5.2 Sampling Algorithm
In order to analyze WALKTREES, we focus on two different measures of regret:
1. Ë†R, the regret measured by the algorithm.
2. R, the underlying regret (if all states were visited every iteration).
In this implementation, the range of counterfactual utilities can be âˆ†u,1 in almost every state. Define CT (I) to
be the number of times an information set I was visited until time T : in particular, how many times the bucket
sequence that makes I reachable was selected. Blackwellâ€™s Approachability Theorem yields us:
Ë†RT
1 (I) â‰¤ âˆ†u,1(I)p|A(I)|pCT (I)
T (26)
Note that this is the average regret bound for CT (I) iterations averaged over T iterations. Observe that for
any B âˆˆ B1 (see Section A.5.1), P
IâˆˆB CT (I) = T . Define Y = maxBâˆˆB1 |B|, in other words the
number of card partitions on the river. Then P
IâˆˆB
pCT (I) â‰¤ âˆšY T (this is because, for arbitrary m, n,
12
(a1 . . . am) âˆˆ Rm, where ai â‰¥ 0 and Pm
i=1 ai = n, Pm
i=1
âˆšai â‰¤ âˆšmn).
X
IâˆˆB
Ë†RT (I) â‰¤ X
IâˆˆB
âˆ†u,1
p|A(I)|pCT (I)
T (27)
X
IâˆˆB
Ë†RT (I) â‰¤
p|A(B)|âˆ†u,1
âˆšY
âˆšT (28)
Ë†RT
1 â‰¤ âˆ†u,1
âˆšY
âˆšT
X
BâˆˆB1
p|A(B)| (29)
Ë†RT
1 â‰¤ âˆ†u,1
âˆšY
âˆšT |B1|p|A1| (30)
Thus, the regret bound has increased by a factor of âˆšY : however, the computation per round has decreased by
a factor of nearly Y 2, resulting in a dramatic overall gain, so long as R and Ë†R are similar.
This last portion is tricky: since the algorithm is randomized, we cannot guarantee that every information set is
reached, let alone that it has converged. Therefore, instead of proving a bound on the absolute difference of R
and Ë†R, we focus on proving a probabilistic connection.
In particular, we will focus on the similarity of the counterfactual regret (RT (I) and Ë†RT (I)) in every node.
In particular, we will focus on the similarity of the counterfactual regret of a particular action at a particular
time (rt
1(I, a) and Ë†rt
1(I, a)). Define Reacht(I) to be true if I is reachable given the actions of nature at time t.
Formally:
rt
1(I, a) = Ï€Ïƒt
âˆ’1(I) `u1(Ïƒt|Iâ†’a, I) âˆ’ u1(Ïƒt, I) Ì (31)
Ë†rt
1(I, a) =
( rt
1(I,a)
Ï€0(I) if Reacht(I)
0 otherwise (32)
It is the case that E[rt
1(I, a) âˆ’ Ë†rt
1(I, a)] = 0. These are the elementary components of RT
1 (I) and Ë†RT (I),
because:
RT
1 (I) = 1
T max
aâˆˆA(I)
TX
t=1
rt
1(I, a) (33)
Ë†RT
1 (I) = 1
T max
aâˆˆA(I)
TX
t=1
Ë†rt
1(I, a) (34)
We bound the expected squared difference between P
IâˆˆIi RT
1 (I) and P
IâˆˆIi
Ë†RT
1 (I) in order to prove that
they are close, because for any random variable X:
Pr[|X| â‰¥ kpE[X2]] â‰¤ 1
k2 (35)
by Markovâ€™s Inequality.
E[( X
IâˆˆI1
(RT
1 (I) âˆ’ Ë†RT
1 (I)))2] â‰¤ |I1| X
IâˆˆI1
E[(RT
1 (I) âˆ’ Ë†RT
1 (I))2] (36)
This is because, for all a1 . . . ak âˆˆ R, (Pk
i=1 ai)2 â‰¤ k Pk
i=1 a2
i . Finally:
(RT
1 (I) âˆ’ Ë†RT
1 (I))2 = 1
T max
aâˆˆA(I)
TX
t=1
rt
1(I, a) âˆ’ 1
T max
aâˆˆA(I)
TX
t=1
Ë†rt
1(I, a)
!2
(37)
(RT
1 (I) âˆ’ Ë†RT
1 (I))2 â‰¤ 1
T 2 max
aâˆˆA(I)
TX
t=1
rt
1(I, a) âˆ’
TX
t=1
Ë†rt
1(I, a)
!2
(38)
(RT
1 (I) âˆ’ Ë†RT
1 (I))2 â‰¤ 1
T 2
X
aâˆˆA(I)
TX
t=1
rt
1(I, a) âˆ’
TX
t=1
Ë†rt
1(I, a)
!2
(39)
E[(RT
1 (I) âˆ’ Ë†RT
1 (I))2] â‰¤ 1
T 2
X
aâˆˆA(I)
TX
t=1
E[`rt
1(I, a) âˆ’ Ë†rt
1(I, a) Ì2] (40)
13
The final step is because if t 6 = tâ€², then E[`rt
1(I, a) âˆ’ Ë†rt
1(I, a) Ì â€œ
rtâ€²
1 (I, a) âˆ’ Ë†rtâ€²
1 (I, a)
â€
] = 0. Substituting
back into Equation 36:
E[( X
IâˆˆI1
(RT
1 (I) âˆ’ Ë†RT
1 (I)))2] â‰¤ |I1|
T 2
X
IâˆˆI1
X
aâˆˆA(I)
TX
t=1
E[`rt
1(I, a) âˆ’ Ë†rt
1(I, a) Ì2] (41)
Recall that Ï€Ïƒt
âˆ’1(I) = Ï€Ïƒt
2 (I)Ï€Ïƒt
0 (I). Thus, |rt
1(I, a)| â‰¤ âˆ†u,1Ï€Ïƒt
0 (I), and Ë†rt
1(I, a) â‰¤ âˆ†u,1. Also,
Pr[Ë†rt
1(I, a) 6 = 0] â‰¤ Ï€0(I). Finally:
E[(rt
1(I, a) âˆ’ Ë†rt
1(I, a))2|Reach(I)] â‰¤ 2âˆ†2
u,1 (42)
E[(rt
1(I, a) âˆ’ Ë†rt
1(I, a))2|Â¬Reach(I)] â‰¤ 2Ï€0(I)âˆ†2
u,1 (43)
E[(rt
1(I, a) âˆ’ Ë†rt
1(I, a))2] â‰¤ 4Ï€0(I)âˆ†2
u,1 (44)
Thus, substituting back into Equation 41:
E[( X
IâˆˆI1
(RT
1 (I) âˆ’ Ë†RT
1 (I)))2] â‰¤ |I1|
T 2
X
IâˆˆI1
X
aâˆˆA(I)
TX
t=1
4Ï€0(I)âˆ†2
u,1 (45)
E[( X
IâˆˆI1
(RT
1 (I) âˆ’ Ë†RT
1 (I)))2] â‰¤ 4|I1|âˆ†2
u,1
T
X
IâˆˆI1
|A(I)|Ï€0(I) (46)
(47)
Again, by focusing on Bi:
E[( X
IâˆˆI1
(RT
1 (I) âˆ’ Ë†RT
1 (I)))2] â‰¤ 4|I1|âˆ†2
u,1
T
X
BâˆˆB1
X
IâˆˆB
|A(I)|Ï€0(I) (48)
E[( X
IâˆˆI1
(RT
1 (I) âˆ’ Ë†RT
1 (I)))2] â‰¤ 4|I1|âˆ†2
u,1
T
X
BâˆˆB1
|A(B)| (49)
For any p âˆˆ [0, 1], with probability at least 1 âˆ’ p:
RT
1 â‰¤ 2p|I1||B1||A1|âˆ†u,1
âˆšpT + âˆ†u,1
âˆšY
âˆšT |B1|p|A1| (50)
14