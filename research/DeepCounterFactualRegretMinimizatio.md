Abstract
Counterfactual Regret Minimization (CFR)is the
leading framework for solving large imperfect-
information games. It converges to an equilibrium
by iteratively traversing the game tree. In order
to deal with extremely large games, abstraction
is typically applied before running CFR. The ab-
stracted game is solved with tabular CFR, and its
solution is mapped back to the full game. This
process can be problematic because aspects of
abstraction are often manual and domain specific,
abstraction algorithms may miss important strate-
gic nuances of the game, and there is a chicken-
and-egg problem because determining a good ab-
straction requires knowledge of the equilibrium
of the game. This paper introducesDeep Counter-
factual Regret Minimization, a form of CFR that
obviates the need for abstraction by instead using
deep neural networks to approximate the behavior
of CFR in the full game. We show that Deep CFR
is principled and achieves strong performance in
large poker games. This is the first non-tabular
variant of CFR to be successful in large games.
1. Introduction
Imperfect-information games model strategic interactions
between multiple agents with only partial information. They
are widely applicable to real-world domains such as negoti-
ations, auctions, and cybersecurity interactions. Typically
in such games, one wishes to find an approximate equilib-
rium in which no player can improve by deviating from the
equilibrium.

The most successful family of algorithms for imperfect-
information games have been variants ofCounterfactual
Regret Minimization (CFR)(Zinkevich et al., 2007). CFR is
an iterative algorithm that converges to a Nash equilibrium

*Equal contribution (^1) Facebook AI Research (^2) Computer Science
Department, Carnegie Mellon University^3 Strategic Machine Inc.,
Strategy Robot Inc., and Optimized Markets Inc. Correspondence
to: Noam Brownnoamb@cs.cmu.edu.
Proceedings of the 36 thInternational Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).
in two-player zero-sum games. Forms of tabular CFR have
been used in all recent milestones in the benchmark domain
of poker (Bowling et al., 2015; MoravË‡c ÌÄ±k et al., 2017; Brown
& Sandholm, 2017) and have been used in all competitive
agents in the Annual Computer Poker Competition going
back at least six years.^1 In order to deal with extremely
large imperfect-information games,abstractionis typically
used to simplify a game by bucketing similar states together
and treating them identically. The simplified (abstracted)
game is approximately solved via tabular CFR. However,
constructing an effective abstraction requires extensive do-
main knowledge and the abstract solution may only be a
coarse approximation of a true equilibrium.
In constrast, reinforcement learning has been successfully
extended to large state spaces by using function approx-
imation with deep neural networks rather than a tabular
representation of the policy (deep RL). This approach has
led to a number of recent breakthroughs in constructing
strategies in large MDPs (Mnih et al., 2015) as well as in
zero-sum perfect-information games such as Go (Silver
et al., 2017; 2018).^2 Importantly, deep RL can learn good
strategies with relatively little domain knowledge for the
specific game (Silver et al., 2017). However, most popular
RL algorithms do not converge to good policies (equilibria)
in imperfect-information games in theory or in practice.
Rather than use tabular CFR with abstraction, this paper
introduces a form of CFR, which we refer to asDeep Coun-
terfactual Regret Minimization, that uses function approx-
imation with deep neural networks to approximate the be-
havior of tabular CFR on the full, unabstracted game. We
prove that Deep CFR converges to an-Nash equilibrium
in two-player zero-sum games and empirically evaluate per-
formance in poker variants, including heads-up limit Texas
holdâ€™em. We show Deep CFR outperforms Neural Ficti-
tious Self Play (NFSP) (Heinrich & Silver, 2016), which
was the prior leading function approximation algorithm for
imperfect-information games, and that Deep CFR is com-
petitive with domain-specific tabular abstraction techniques.
(^1) http://www.computerpokercompetition.org
(^2) Deep RL has also been applied successfully to some partially
observed games such as Doom (Lample & Chaplot, 2017), as long
as the hidden information is not too strategically important.

2. Notation and Background
In an imperfect-information extensive-form (that is, tree-
form) game there is a finite set of players,P. Anode
(or history)his defined by all information of the current
situation, including private knowledge known to only one
player. A(h)denotes the actions available at a node and
P(h)is either chance or the unique player who acts at that
node. If actionaâˆˆA(h)leads fromhtohâ€², then we write
hÂ·a=hâ€². We writeh@hâ€²if a sequence of actions leads
fromhtohâ€².His the set of all nodes.ZâŠ†Hare terminal
nodes for which no actions are available. For each player
pâˆˆ P, there is a payoff functionup:Zâ†’R. In this
paper we assumeP={ 1 , 2 }andu 1 =âˆ’u 2 (the game is
two-player zero-sum). We denote the range of payoffs in
the game byâˆ†.

Imperfect information is represented byinformation sets
(infosets) for each playerpâˆˆ P. For any infosetIbe-
longing top, all nodesh,hâ€²âˆˆIare indistinguishable to
p. Moreover, every non-terminal nodehâˆˆHbelongs to
exactly one infoset for eachp. We represent the set of all
infosets belonging topwherepacts byIp. We call the
set of all terminal nodes with a prefix inIasZI, and we
call the particular prefixz[I]. We assume the game features
perfect recall, which means ifhandhâ€²do not share a player
pinfoset then all nodes followinghdo not share a playerp
infoset with any node followinghâ€².

A strategy (or policy)Ïƒ(I)is a probability vector over ac-
tions for acting playerpin infosetI. Since all states in an
infoset belonging topare indistinguishable, the strategies
in each of them must be identical. The set of actions inIis
denoted byA(I). The probability of a particular actionais
denoted byÏƒ(I,a). We defineÏƒpto be a strategy forpin
every infoset in the game wherepacts. A strategy profileÏƒ
is a tuple of strategies, one for each player. The strategy of
every player other thanpis represented asÏƒâˆ’p.up(Ïƒp,Ïƒâˆ’p)
is the expected payoff forpif playerpplays according to
Ïƒpand the other players play according toÏƒâˆ’p.

Ï€Ïƒ(h) = Î hâ€²Â·avhÏƒP(hâ€²)(hâ€²,a)is calledreachand is the
probabilityhis reached if all players play according toÏƒ.
Ï€Ïƒp(h)is the contribution ofpto this probability.Ï€âˆ’Ïƒp(h)
is the contribution of chance and all players other thanp.
For an infosetIbelonging top, the probability of reaching
Iifpchooses actions leading towardIbut chance and all
players other thanpplay according toÏƒâˆ’pis denoted by
Ï€Ïƒâˆ’p(I) =

âˆ‘
hâˆˆIÏ€
Ïƒ
âˆ’p(h). Forhvz, defineÏ€
Ïƒ(hâ†’z) =
Î hâ€²Â·avz,hâ€² 6 @hÏƒP(hâ€²)(hâ€²,a)

Abest responsetoÏƒâˆ’pis a playerpstrategyBR(Ïƒâˆ’p)
such thatup

(
BR(Ïƒâˆ’p),Ïƒâˆ’p
)
= maxÏƒâ€²pup(Ïƒâ€²p,Ïƒâˆ’p). A
Nash equilibrium Ïƒâˆ— is a strategy profile where ev-
eryone plays a best response: âˆ€p, up(Ïƒâˆ—p,Ïƒâˆ—âˆ’p) =
maxÏƒâ€²pup(Ïƒâ€²p,Ïƒâˆ—âˆ’p)(Nash, 1950). Theexploitabilitye(Ïƒp)

of a strategyÏƒpin a two-player zero-sum game is how much
worseÏƒpdoes versusBR(Ïƒp)compared to how a Nash
equilibrium strategyÏƒpâˆ—does againstBR(Ïƒâˆ—p). Formally,
e(Ïƒp) =up
(
Ïƒâˆ—p,BR(Ïƒpâˆ—)
)
âˆ’up
(
Ïƒp,BR(Ïƒp)
)
. We mea-
suretotal exploitability

âˆ‘
pâˆˆPe(Ïƒp)
(^3).
2.1. Counterfactual Regret Minimization (CFR)
CFR is an iterative algorithm that converges to a Nash equi-
librium in any finite two-player zero-sum game with a the-
oretical convergence bound ofO(âˆš^1 T). In practice CFR
converges much faster. We provide an overview of CFR be-
low; for a full treatment, see Zinkevich et al. (2007). Some
recent forms of CFR converge inO(T 01. 75 )in self-play set-
tings (Farina et al., 2019), but are slower in practice so we
do not use them in this paper.
LetÏƒtbe the strategy profile on iterationt. Thecounter-
factual valuevÏƒ(I)of playerp=P(I)atIis the expected
payoff topwhen reachingI, weighted by the probability
thatpwould reachedIif she tried to do so that iteration.
Formally,
vÏƒ(I) =

âˆ‘
zâˆˆZI
Ï€âˆ’Ïƒp(z[I])Ï€Ïƒ(z[I]â†’z)up(z) (1)
andvÏƒ(I,a)is the same except it assumes that playerp
plays actionaat infosetIwith 100% probability.
Theinstantaneous regretrt(I,a)is the difference between
P(I)â€™s counterfactual value from playingavs. playingÏƒ
on iterationt
rt(I,a) =vÏƒ
t
(I,a)âˆ’vÏƒ
t
(I) (2)
Thecounterfactual regretfor infosetIactionaon iteration
Tis
RT(I,a) =
âˆ‘T
t=
rt(I,a) (3)
Additionally,RT+(I,a) = max{RT(I,a), 0 }andRT(I) =
maxa{RT(I,a)}.Total regretforpin the entire game is
RTp= maxÏƒâ€²p
âˆ‘T
t=
(
up(Ïƒpâ€²,Ïƒtâˆ’p)âˆ’up(Ïƒtp,Ïƒtâˆ’p)
)
.
CFR determines an iterationâ€™s strategy by applying any of
severalregret minimizationalgorithms to each infoset (Lit-
tlestone & Warmuth, 1994; Chaudhuri et al., 2009). Typi-
cally,regret matching(RM) is used as the regret minimiza-
tion algorithm within CFR due to RMâ€™s simplicity and lack
of parameters (Hart & Mas-Colell, 2000).
In RM, a player picks a distribution over actions in an in-
foset in proportion to the positive regret on those actions.
Formally, on each iterationt+ 1,pselects actionsaâˆˆA(I)
(^3) Some prior papers instead measureaverageexploitability
rather thantotal(summed) exploitability.

according to probabilities

Ïƒt+1(I,a) =
Rt+(I,a)
âˆ‘
aâ€²âˆˆA(I)R
t
+(I,aâ€²)
(4)
If

âˆ‘
aâ€²âˆˆA(I)R
t
+(I,a
â€²) = 0then any arbitrary strategy may
be chosen. Typically each action is assigned equal proba-
bility, but in this paper we choose the action with highest
counterfactual regret with probability 1 , which we find em-
pirically helps RM better cope with approximation error
(see Figure 4).

If a player plays according to regret matching in in-
fosetIon every iteration, then on iterationT,RT(I)â‰¤
âˆ†

âˆš
|A(I)|
âˆš
T(Cesa-Bianchi & Lugosi, 2006). Zinkevich
et al. (2007) show that the sum of the counterfactual regret
across all infosets upper bounds the total regret. Therefore,
if playerpplays according to CFR on every iteration, then

RTpâ‰¤

âˆ‘
IâˆˆIpR
T(I). So, asTâ†’âˆ,R
Tp
T â†’^0.
The average strategyÏƒ Ì„pT(I)for an infosetIon iterationT

is Ì„ÏƒpT(I) =

âˆ‘T
t=
(
Ï€Ïƒpt(I)Ïƒpt(I)
)
âˆ‘T
t=1Ï€Ïƒ
pt(I).
In two-player zero-sum games, if both playersâ€™ average

total regret satisfies
RTp
T â‰¤, then their average strategies
ã€ˆÏƒ Ì„T 1 ,Ïƒ Ì„T 2 ã€‰form a 2 -Nash equilibrium (Waugh, 2009). Thus,
CFR constitutes an anytime algorithm for finding an-Nash
equilibrium in two-player zero-sum games.

In practice, faster convergence is achieved by alternating
which player updates their regrets on each iteration rather
than updating the regrets of both players simultaneously
each iteration, though this complicates the theory (Farina
et al., 2018; Burch et al., 2018). We use the alternating-
updates form of CFR in this paper.

2.2. Monte Carlo Counterfactual Regret Minimization

Vanilla CFR requires full traversals of the game tree, which
is infeasible in large games. One method to combat this is
Monte Carlo CFR (MCCFR), in which only a portion of
the game tree is traversed on each iteration (Lanctot et al.,
2009). In MCCFR, a subset of nodesQtin the game tree is
traversed at each iteration, whereQtis sampled from some
distributionQ. Sampled regretsr Ìƒtare tracked rather than
exact regrets. For infosets that are sampled at iterationt,
r Ìƒt(I,a)is equal tort(I,a)divided by the probability of
having sampledI; for unsampled infosets Ìƒrt(I,a) = 0. See
Appendix B for more details.

There exist a number of MCCFR variants (Gibson et al.,
2012; Johanson et al., 2012; Jackson, 2017), but for this
paper we focus specifically on theexternal samplingvariant
due to its simplicity and strong performance. In external-
sampling MCCFR the game tree is traversed for one player
at a time, alternating back and forth. We refer to the player

who is traversing the game tree on the iteration as thetra-
verser. Regrets are updated only for the traverser on an
iteration. At infosets where the traverser acts, all actions are
explored. At other infosets and chance nodes, only a single
action is explored.
External-sampling MCCFR probabilistically converges to
an equilibrium. For anyÏâˆˆ(0,1], total regret is bounded
byRpTâ‰¤
(
1 +
âˆš
âˆš^2
Ï
)
|Ip|âˆ†
âˆš
|A|
âˆš
Twith probability 1 âˆ’Ï.
3. Related Work
CFR is not the only iterative algorithm capable of solving
large imperfect-information games. First-order methods
converge to a Nash equilibrium inO(1/T)(Hoda et al.,
2010; Kroer et al., 2018b;a), which is far better than CFRâ€™s
theoretical bound. However, in practice the fastest variants
of CFR are substantially faster than the best first-order meth-
ods. Moreover, CFR is more robust to error and therefore
likely to do better when combined with function approxima-
tion.
Neural Fictitious Self Play (NFSP) (Heinrich & Silver,
2016) previously combined deep learning function approx-
imation with Fictitious Play (Brown, 1951) to produce an
AI for heads-up limit Texas holdâ€™em, a large imperfect-
information game. However, Fictitious Play has weaker
theoretical convergence guarantees than CFR, and in prac-
tice converges slower. We compare our algorithm to NFSP
in this paper. Model-free policy gradient algorithms have
been shown to minimize regret when parameters are tuned
appropriately (Srinivasan et al., 2018) and achieve perfor-
mance comparable to NFSP.
Past work has investigated using deep learning to esti-
mate values at the depth limit of a subgame in imperfect-
information games (MoravË‡c ÌÄ±k et al., 2017; Brown et al.,
2018). However, tabular CFR was used within the sub-
games themselves. Large-scale function approximated CFR
has also been developed for single-agent settings (Jin et al.,
2017). Our algorithm is intended for the multi-agent set-
ting and is very different from the one proposed for the
single-agent setting.
Prior work has combined regression tree function approxi-
mation with CFR (Waugh et al., 2015) in an algorithm called
Regression CFR (RCFR). This algorithm defines a number
of features of the infosets in a game and calculates weights
to approximate the regrets that a tabular CFR implemen-
tation would produce. Regression CFR is algorithmically
similar to Deep CFR, but uses hand-crafted features similar
to those used in abstraction, rather than learning the features.
RCFR also uses full traversals of the game tree (which is
infeasible in large games) and has only been evaluated on
toy games. It is therefore best viewed as the first proof of
concept that function approximation can be applied to CFR.
Concurrent work has also investigated a similar combina-
tion of deep learning with CFR, in an algorithm referred
to as Double Neural CFR (Li et al., 2018). However, that
approach may not be theoretically sound and the authors
consider only small games. There are important differences
between our approaches in how training data is collected
and how the behavior of CFR is approximated.

4. Description of the Deep Counterfactual
Regret Minimization Algorithm
In this section we describe Deep CFR. The goal of Deep
CFR is to approximate the behavior of CFR without calcu-
lating and accumulating regrets at each infoset, by general-
izing across similar infosets using function approximation
via deep neural networks.

On each iterationt, Deep CFR conducts a constant num-
berKof partial traversals of the game tree, with the path
of the traversal determined according to external sampling
MCCFR. At each infosetIit encounters, it plays a strategy
Ïƒt(I)determined by regret matching on the output of a neu-
ral networkV:Iâ†’R|A|defined by parametersÎ¸tpâˆ’^1 that
takes as input the infosetIand outputs valuesV(I,a|Î¸tâˆ’^1 ).
Our goal is forV(I,a|Î¸tâˆ’^1 )to be approximately propor-
tional to the regretRtâˆ’^1 (I,a)that tabular CFR would have
produced.

When a terminal node is reached, the value is passed back up.
In chance and opponent infosets, the value of the sampled
action is passed back up unaltered. In traverser infosets, the
value passed back up is the weighted average of all action
values, where actionaâ€™s weight isÏƒt(I,a). This produces
samples of this iterationâ€™s instantaneous regrets for various
actions. Samples are added to a memoryMv,p, wherep
is the traverser, using reservoir sampling (Vitter, 1985) if
capacity is exceeded.

Consider a nice property of the sampled instantaneous re-
grets induced by external sampling:

Lemma 1. For external sampling MCCFR, the sampled
instantaneous regrets are an unbiased estimator of thead-
vantage, i.e. the difference in expected payoff for playing
avsÏƒtp(I)atI, assuming both players playÏƒteverywhere
else.

EQâˆˆQt
[
ÌƒrÏƒ
t
p(I,a)
âˆ£
âˆ£
âˆ£ZIâˆ©Q^6 =âˆ…
]
=
vÏƒ
t
(I,a)âˆ’vÏƒ
t
(I)
Ï€Ïƒ
t
âˆ’p(I)
.
The proof is provided in Appendix B.2.

Recent work in deep reinforcement learning has shown
that neural networks can effectively predict and generalize
advantages in challenging environments with large state
spaces, and use that to learn good policies (Mnih et al.,
2016).

Once a playerâ€™sKtraversals are completed, a new network
is trainedfrom scratchto determine parametersÎ¸tpby mini-
mizing MSE between predicted advantageVp(I,a|Î¸t)and
samples of instantaneous regrets from prior iterationstâ€²â‰¤t
r Ìƒt
â€²
(I,a)drawn from the memory. The average over all
sampled instantaneous advantages Ìƒrt
â€²
(I,a)is proportional
to the total sampled regretR Ìƒt(I,a)(across actions in an
infoset), so once a sample is added to the memory it is never
removed except through reservoir sampling, even when the
next CFR iteration begins.
One can use any loss function for the value and average
strategy model that satisfies Bregman divergence (Banerjee
et al., 2005), such as mean squared error loss.
While almost any sampling scheme is acceptable so long
as the samples are weighed properly, external sampling
has the convenient property that it achieves both of our
desired goals by assigning all samples in an iteration equal
weight. Additionally, exploring all of a traverserâ€™s actions
helps reduce variance. However, external sampling may
be impractical in games with extremely large branching
factors, so a different sampling scheme, such as outcome
sampling (Lanctot et al., 2009), may be desired in those
cases.
In addition to the value network, a separate policy network
Î  :Iâ†’R|A|approximates the average strategy at the end
of the run, because it is theaverage strategy played over all
iterationsthat converges to a Nash equilibrium. To do this,
we maintain a separate memoryMÎ of sampled infoset
probability vectors for both players. Whenever an infoset
Ibelonging to playerpis traversed during the opposing
playerâ€™s traversal of the game tree via external sampling,
the infoset probability vectorÏƒt(I)is added toMÎ and
assigned weightt.
If the number of Deep CFR iterations and the size of each
value network model is small, then one can avoid training
the final policy network by instead storing each iterationâ€™s
value network (Steinberger, 2019). During actual play, a
value network is sampled randomly and the player plays the
CFR strategy resulting from the predicted advantages of that
network. This eliminates the function approximation error
of the final average policy network, but requires storing all
prior value networks. Nevertheless, strong performance and
low exploitability may still be achieved by storing only a
subset of the prior value networks (Jackson, 2016).
Theorem 1 states that if the memory buffer is sufficiently
large, then with high probability Deep CFR will result in
average regret being bounded by a constant proportional to
the square root of the function approximation error.
Theorem 1.LetTdenote the number of Deep CFR itera-
tions,|A|the maximum number of actions at any infoset,
andKthe number of traversals per iteration. LetLtVbe
the average MSE loss forVp(I,a|Î¸t)on a sample inMV,p
at iterationt, and letLtVâˆ—be the minimum loss achievable
for any functionV. LetLtVâˆ’LtVâˆ—â‰¤L.

If the value memories are sufficiently large, then with proba-
bility 1 âˆ’Ïtotal regret at timeTis bounded by

RTpâ‰¤
(
1 +
âˆš
2
âˆš
ÏK
)
âˆ†|Ip|
âˆš
|A|
âˆš
T+ 4T|Ip|
âˆš
|A|âˆ†L
(5)
with probability 1 âˆ’Ï.

Corollary 1.AsTâ†’âˆ, average regret
RTp
T is bounded by

4 |Ip|
âˆš
|A|âˆ†L
with high probability.

The proofs are provided in Appendix B.4.

We do not provide a convergence bound for Deep CFR when
using linear weighting, since the convergence rate of Linear
CFR has not been shown in the Monte Carlo case. However,
Figure 4 shows moderately faster convergence in practice.

5. Experimental Setup
We measure the performance of Deep CFR (Algorithm 1 )
in approximating an equilibrium in heads-up flop holdâ€™em
poker (FHP). FHP is a large game with over 1012 nodes
and over 109 infosets. In contrast, the network we use has
98,948 parameters. FHP is similar to heads-up limit Texas
holdâ€™em (HULH) poker, but ends after the second betting
round rather than the fourth, with only three community
cards ever dealt. We also measure performance relative to
domain-specific abstraction techniques in the benchmark
domain of HULH poker, which has over 1017 nodes and
over 1014 infosets. The rules for FHP and HULH are given
in Appendix A.

In both games, we compare performance to NFSP, which
is the previous leading algorithm for imperfect-information
game solving using domain-independent function approx-
imation, as well as state-of-the-art abstraction techniques
designed for the domain of poker (Johanson et al., 2013;
Ganzfried & Sandholm, 2014; Brown et al., 2015).

5.1. Network Architecture

We use the neural network architecture shown in Figure 5.
for both the value networkVthat computes advantages for
each player and the networkÎ that approximates the final
average strategy. This network has a depth of 7 layers and
98,948 parameters. Infosets consist of sets of cards and
bet history. The cards are represented as the sum of three
embeddings: a rank embedding (1-13), a suit embedding

Figure 1.The neural network architecture used for Deep CFR.
The network takes an infoset (observed cards and bet history) as
input and outputs values (advantages or probability logits) for each
possible action.
(1-4), and a card embedding (1-52). These embeddings
are summed for each set of permutation invariant cards
(hole, flop, turn, river), and these are concatenated. In
each of theNroundsrounds of betting there can be at most 6
sequential actions, leading to 6 Nroundstotal unique betting
positions. Each betting position is encoded by a binary
value specifying whether a bet has occurred, and a float
value specifying the bet size.
The neural network model begins with separate branches for
the cards and bets, with three and two layers respectively.
Features from the two branches are combined and three
additional fully connected layers are applied. Each fully-
connected layer consists ofxi+1=ReLU(Ax[+x]). The
optional skip connection[+x]is applied only on layers that
have equal input and output dimension. Normalization (to
zero mean and unit variance) is applied to the last-layer
features. The network architecture was not highly tuned, but
normalization and skip connections were used because they
were found to be important to encourage fast convergence
when running preliminary experiments on pre-computed
equilibrium strategies in FHP. A full network specification
is provided in Appendix C.
In the value network, the vector of outputs represented pre-
dicted advantages for each action at the input infoset. In the
average strategy network, outputs are interpreted as logits
of the probability distribution over actions.
5.2. Model training
We allocate a maximum size of 40 million infosets to each
playerâ€™s advantage memoryMV,pand the strategy memory
MÎ . The value model is trained from scratch each CFR
iteration, starting from a random initialization. We perform
4,000 mini-batch stochastic gradient descent (SGD) itera-
tions using a batch size of 10,000 and perform parameter
updates using the Adam optimizer (Kingma & Ba, 2014)
with a learning rate of 0. 001 , with gradient norm clipping
to 1. For HULH we use 32,000 SGD iterations and a batch
size of 20,000. Figure 4 shows that training the model from
Algorithm 1Deep Counterfactual Regret Minimization
functionDEEPCFR
Initialize each playerâ€™s advantage networkV(I,a|Î¸p)with parametersÎ¸pso that it returns 0 for all inputs.
Initialize reservoir-sampled advantage memoriesMV, 1 ,MV, 2 and strategy memoryMÎ .
forCFR iterationt= 1toTdo
for eachplayerpdo
fortraversalk= 1toKdo
TRAVERSE(âˆ…,p,Î¸ 1 ,Î¸ 2 ,MV,p,MÎ ) .Collect data from a game traversal with external sampling
TrainÎ¸pfrom scratch on lossL(Î¸p) =E(I,tâ€²,r Ìƒtâ€²)âˆ¼MV,p

[
tâ€²
âˆ‘
a
(
r Ìƒt
â€²
(a)âˆ’V(I,a|Î¸p)
) 2 ]
TrainÎ¸Î on lossL(Î¸Î ) =E(I,tâ€²,Ïƒtâ€²)âˆ¼MÎ 
[
tâ€²
âˆ‘
a
(
Ïƒt
â€²
(a)âˆ’Î (I,a|Î¸Î )
) 2 ]
returnÎ¸Î 
Algorithm 2CFR Traversal with External Sampling
functionTRAVERSE(h,p,Î¸ 1 ,Î¸ 2 ,MV,MÎ , t)
Input:Historyh, traverser playerp, regret network parametersÎ¸for each player, advantage memoryMVfor player
p, strategy memoryMÎ , CFR iterationt.

ifhis terminalthen
returnthe payoff to playerp
else ifhis a chance nodethen
aâˆ¼Ïƒ(h)
returnTRAVERSE(hÂ·a,p,Î¸ 1 ,Î¸ 2 ,MV,MÎ , t)
else ifP(h) =pthen .If itâ€™s the traverserâ€™s turn to act
Compute strategyÏƒt(I)from predicted advantagesV(I(h),a|Î¸p)using regret matching.
foraâˆˆA(h)do
v(a)â†TRAVERSE(hÂ·a,p,Î¸ 1 ,Î¸ 2 ,MV,MÎ , t) .Traverse each action
foraâˆˆA(h)do
r Ìƒ(I,a)â†v(a)âˆ’
âˆ‘
aâ€²âˆˆA(h)Ïƒ(I,a
â€²)Â·v(aâ€²) .Compute advantages
Insert the infoset and its action advantages(I,t,r Ìƒt(I))into the advantage memoryMV
else .If itâ€™s the opponentâ€™s turn to act
Compute strategyÏƒt(I)from predicted advantagesV(I(h),a|Î¸ 3 âˆ’p)using regret matching.
Insert the infoset and its action probabilities(I,t,Ïƒt(I))into the strategy memoryMÎ 
Sample an actionafrom the probability distributionÏƒt(I).
returnTRAVERSE(hÂ·a,p,Î¸ 1 ,Î¸ 2 ,MV,MÎ , t)
scratch at each iteration, rather than using the weights from
the previous iteration, leads to better convergence.

5.3. Linear CFR

There exist a number of variants of CFR that achieve much
faster performance than vanilla CFR. However, most of
these faster variants of CFR do not handle approximation
error well (Tammelin et al., 2015; Burch, 2017; Brown &
Sandholm, 2019; Schmid et al., 2019). In this paper we use
Linear CFR (LCFR)(Brown & Sandholm, 2019), a variant
of CFR that is faster than CFR and in certain settings is
the fastest-known variant of CFR (particularly in settings
with wide distributions in payoffs), and which tolerates
approximation error well. LCFR is not essential and does

not appear to lead to better performance asymptotically, but
does result in faster convergence in our experiments.
LCFR is like CFR except iterationtis weighed byt. Specif-
ically, we maintain aweighton each entry stored in the
advantage memory and the strategy memory, equal tot
when this entry was added. When trainingÎ¸peach itera-
tionT, we rescale all the batch weights byT^2 and minimize
weighted error.
6. Experimental Results
Figure 2 compares the performance of Deep CFR to
different-sized domain-specific abstractions in FHP. The ab-
stractions are solved using external-sampling Linear Monte
Carlo CFR (Lanctot et al., 2009; Brown & Sandholm, 2019),
which is the leading algorithm in this setting. The 40,
cluster abstraction means that the more than 109 different
decisions in the game were clustered into 40,000 abstract
decisions, where situations in the same bucket are treated
identically. This bucketing is done using K-means clustering
on domain-specific features. Thelossless abstractiononly
clusters together situations that are strategically isomorphic
(e.g., flushes that differ only by suit), so a solution to this
abstraction maps to a solution in the full game without error.

Performance and exploitability are measured in terms of
milli big blinds per game (mbb/g), which is a standard
measure of win rate in poker.

The figure shows that Deep CFR asymptotically reaches a
similar level of exploitability as the abstraction that uses 3.
million clusters, but converges substantially faster. Although
Deep CFR is more efficient in terms of nodes touched, neu-
ral network inference and training requires considerable
overhead that tabular CFR avoids. However, Deep CFR
does not require advanced domain knowledge. We show
Deep CFR performance for 10,000 CFR traversals per step.
Using more traversals per step is less sample efficient and
requires greater neural network training time but requires
fewer CFR steps.

Figure 2 also compares the performance of Deep CFR to
NFSP, an existing method for learning approximate Nash
equilibria in imperfect-information games. NFSP approx-
imates fictitious self-play, which is proven to converge to
a Nash equilibrium but in practice does so far slower than
CFR. We observe that Deep CFR reaches an exploitability
of 37 mbb/g while NFSP converges to 47 mbb/g.^4 We also
observe that Deep CFR is more sample efficient than NFSP.
However, these methods spend most of their wallclock time
performing SGD steps, so in our implementation we see a
less dramatic improvement over NFSP in wallclock time
than sample efficiency.

Figure 3 shows the performance of Deep CFR using differ-
ent numbers of game traversals, network SGD steps, and
model size. As the number of CFR traversals per iteration
is reduced, convergence becomes slower but the model con-
verges to the same final exploitability. This is presumably
because it takes more iterations to collect enough data to
reduce the variance sufficiently. On the other hand, reduc-
ing the number of SGD steps does not change the rate of
convergence but affects the asymptotic exploitability of the

(^4) We run NFSP with the same model architecture as we use
for Deep CFR. In the benchmark game of Leduc Holdâ€™em, our
implementation of NFSP achieves an average exploitability (total
exploitability divided by two) of 37 mbb/g in the benchmark game
of Leduc Holdâ€™em, which is substantially lower than originally
reported in Heinrich & Silver (2016). We report NFSPâ€™s best
performance in FHP across a sweep of hyperparameters.
model. This is presumably because the model loss decreases
as the number of training steps is increased per iteration (see
Theorem 1). Increasing the model size also decreases final
exploitability up to a certain model size in FHP.
In Figure 4 we consider ablations of certain components of
Deep CFR. Retraining the regret model from scratch at each
CFR iteration converges to a substantially lower exploitabil-
ity than fine-tuning a single model across all iterations. We
suspect that this is because a single model gets stuck in bad
local minima as the objective is changed from iteration to
iteration. The choice of reservoir sampling to update the
memories is shown to be crucial; if a sliding window mem-
ory is used, the exploitability begins to increase once the
memory is filled up, even if the memory is large enough to
hold the samples from many CFR iterations.
Finally, we measure head-to-head performance in HULH.
We compare Deep CFR and NFSP to the approximate solu-
tions (solved via Linear Monte Carlo CFR) of three different-
sized abstractions: one in which the more than 1014 deci-
sions are clustered into 3. 3 Â· 106 buckets, one in which there
are 3. 3 Â· 107 buckets and one in which there are 3. 3 Â· 108 buck-
ets. The results are presented in Table 1. For comparison,
the largest abstractions used by the poker AI Polaris in its
2007 HULH man-machine competition against human pro-
fessionals contained roughly 3 Â· 108 buckets. When variance-
reduction techniques were applied, the results showed that
the professional human competitors lost to the 2007 Polaris
AI by about 52 Â± 10 mbb/g (Johanson, 2016). In contrast,
our Deep CFR agent loses to a 3. 3 Â· 108 bucket abstraction
by onlyâˆ’ 11 Â± 2 mbb/g and beats NFSP by 43 Â± 2 mbb/g.
(^106107108) Nodes Touched 109 1010 1011
102
103
Exploitability (mbb/g)
Convergence of Deep CFR, NFSP, and Domain-Specific Abstractions
Deep CFR
NFSP (1,000 infosets / update)NFSP (10,000 infosets / update)
Abstraction (40,000 Clusters)Abstraction (368,000 Clusters)
Abstraction (3,644,000 Clusters)Lossless Abstraction (234M Clusters)
Figure 2.Comparison of Deep CFR with domain-specific tabular
abstractions and NFSP in FHP. Coarser abstractions converge faster
but are more exploitable. Deep CFR converges with 2-3 orders of
magnitude fewer samples than a lossless abstraction, and performs
competitively with a 3.6 million cluster abstraction. Deep CFR
achieves lower exploitability than NFSP, while traversing fewer
infosets.

Opponent Model
Abstraction Size
Model NFSP Deep CFR 3. 3 Â· 106 3. 3 Â· 107 3. 3 Â· 108
NFSP - âˆ’ 43 Â± 2 mbb/g âˆ’ 40 Â± 2 mbb/g âˆ’ 49 Â± 2 mbb/g âˆ’ 55 Â± 2 mbb/g
Deep CFR +43Â± 2 mbb/g - +6Â± 2 mbb/g âˆ’ 6 Â± 2 mbb/g âˆ’ 11 Â± 2 mbb/g
Table 1.Head-to-head expected value of NFSP and Deep CFR in HULH against converged CFR equilibria with varying abstraction sizes.
For comparison, in 2007 an AI using abstractions of roughly 3 Â· 108 buckets defeated human professionals by about 52 mbb/g (after
variance reduction techniques were applied).
(^101) CFR Iteration 102
102
103
Exploitability (mbb/g)
Traversals per iter3,
10,00030,
100,000300,
1,000,000Linear CFR
(^101) CFR Iteration 102
102
103
Exploitability (mbb/g)
SGD steps per iter
1,0002,
4,
8,00016,
32,000Linear CFR
(^104) # Model Parameters 105 106
102
Exploitability (mbb/g)
dim=
dim=
dim=
dim=
dim=128dim=
Figure 3.Left:FHP convergence for different numbers of training data collection traversals per simulated LCFR iteration. The dotted
line shows the performance of vanilla tabular Linear CFR without abstraction or sampling.Middle:FHP convergence using different
numbers of minibatch SGD updates to train the advantage model at each LCFR iteration.Right:Exploitability of Deep CFR in FHP for
different model sizes. Label indicates the dimension (number of features) in each hidden layer of the model.
101 102
CFR Iteration
102
103
Exploitability (mbb/g)
Deep CFR (5 replicates)Deep CFR without Linear Weighting
Deep CFR without Retraining from ScratchDeep CFR Playing Uniform when All Regrets < 0
101 102
CFR Iteration
102
103
Exploitability (mbb/g)
Deep CFR
Deep CFR with Sliding Window Memories
Figure 4.Ablations of Deep CFR components in FHP.Left:As a baseline, we plot 5 replicates of Deep CFR, which show consistent
exploitability curves (standard deviation att= 450is 2. 25 mbb/g). Deep CFR without linear weighting converges to a similar
exploitability, but more slowly. If the same network is fine-tuned at each CFR iteration rather than training from scratch, the final
exploitability is about 50% higher. Also, if the algorithm plays a uniform strategy when all regrets are negative (i.e. standard regret
matching), rather than the highest-regret action, the final exploitability is also 50% higher.Right:If Deep CFR is performed using
sliding-window memories, exploitability stops converging once the buffer becomes full^6. However, with reservoir sampling, convergence
continues after the memories are full.

7. Conclusions
We describe a method to find approximate equilibria in
large imperfect-information games by combining the CFR
algorithm with deep neural network function approxima-
tion. This method is theoretically principled and achieves
strong performance in large poker games relative to domain-
specific abstraction techniques without relying on advanced
domain knowledge. This is the first non-tabular variant of
CFR to be successful in large games.
Deep CFR and other neural methods for imperfect-
information games provide a promising direction for tack-
ling large games whose state or action spaces are too large
for tabular methods and where abstraction is not straight-
forward. Extending Deep CFR to larger games will likely
require more scalable sampling strategies than those used in
this work, as well as strategies to reduce the high variance
in sampled payoffs. Recent work has suggested promising
directions both for more scalable sampling (Li et al., 2018)
and variance reduction techniques (Schmid et al., 2019). We
believe these are important areas for future work.
8. Acknowledgments
This material is based on work supported by the National
Science Foundation under grants IIS-1718457, IIS-1617590,
IIS-1901403, and CCF-1733556, and the ARO under award
W911NF-17-1-0082. Noam Brown was partly supported by
an Open Philanthropy Project AI Fellowship and a Tencent
AI Lab fellowship.

References
Banerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J. Clus-
tering with bregman divergences. Journal of machine
learning research, 6(Oct):1705â€“1749, 2005.

Bowling, M., Burch, N., Johanson, M., and Tammelin, O.
Heads-up limit holdâ€™em poker is solved. Science, 347
(6218):145â€“149, January 2015.

Brown, G. W. Iterative solutions of games by fictitious play.
In Koopmans, T. C. (ed.),Activity Analysis of Production
and Allocation, pp. 374â€“376. John Wiley & Sons, 1951.

Brown, N. and Sandholm, T. Superhuman AI for heads-up
no-limit poker: Libratus beats top professionals.Science,
pp. eaao1733, 2017.

Brown, N. and Sandholm, T. Solving imperfect-information
games via discounted regret minimization. InAAAI Con-
ference on Artificial Intelligence (AAAI), 2019.

Brown, N., Ganzfried, S., and Sandholm, T. Hierarchical ab-
straction, distributed equilibrium computation, and post-
processing, with application to a champion no-limit texas
holdâ€™em agent. InProceedings of the 2015 International
Conference on Autonomous Agents and Multiagent Sys-
tems, pp. 7â€“15. International Foundation for Autonomous
Agents and Multiagent Systems, 2015.

Brown, N., Sandholm, T., and Amos, B. Depth-limited
solving for imperfect-information games. InAdvances in
Neural Information Processing Systems, 2018.

Burch, N. Time and Space: Why Imperfect Information
Games are Hard. PhD thesis, University of Alberta, 2017.

Burch, N., Moravcik, M., and Schmid, M. Revisiting cfr+
and alternating updates.arXiv preprint arXiv:1810.11542,

Cesa-Bianchi, N. and Lugosi, G.Prediction, learning, and
games. Cambridge University Press, 2006.

Chaudhuri, K., Freund, Y., and Hsu, D. J. A parameter-free
hedging algorithm. InAdvances in neural information
processing systems, pp. 297â€“305, 2009.

Farina, G., Kroer, C., and Sandholm, T. Online convex opti-
mization for sequential decision processes and extensive-
form games. InAAAI Conference on Artificial Intelli-
gence (AAAI), 2018.
Farina, G., Kroer, C., Brown, N., and Sandholm, T. Stable-
predictive optimistic counterfactual regret minimization.
InInternational Conference on Machine Learning, 2019.
Ganzfried, S. and Sandholm, T. Potential-aware imperfect-
recall abstraction with earth moverâ€™s distance in
imperfect-information games. InAAAI Conference on
Artificial Intelligence (AAAI), 2014.
Gibson, R., Lanctot, M., Burch, N., Szafron, D., and Bowl-
ing, M. Generalized sampling and variance in coun-
terfactual regret minimization. InProceedins of the
Twenty-Sixth AAAI Conference on Artificial Intelligence,
pp. 1355â€“1361, 2012.
Hart, S. and Mas-Colell, A. A simple adaptive procedure
leading to correlated equilibrium. Econometrica, 68:
1127â€“1150, 2000.
Heinrich, J. and Silver, D. Deep reinforcement learning
from self-play in imperfect-information games. arXiv
preprint arXiv:1603.01121, 2016.
Hoda, S., Gilpin, A., Pe Ìƒna, J., and Sandholm, T. Smoothing
techniques for computing Nash equilibria of sequential
games.Mathematics of Operations Research, 35(2):494â€“
512, 2010. Conference version appeared in WINE-07.
Jackson, E. Targeted CFR. InAAAI Workshop on Computer
Poker and Imperfect Information, 2017.
Jackson, E. G. Compact CFR. InAAAI Workshop on Com-
puter Poker and Imperfect Information, 2016.
Jin, P. H., Levine, S., and Keutzer, K. Regret minimiza-
tion for partially observable deep reinforcement learning.
arXiv preprint arXiv:1710.11424, 2017.
Johanson, M., Bard, N., Lanctot, M., Gibson, R., and
Bowling, M. Efficient nash equilibrium approximation
through monte carlo counterfactual regret minimization.
InProceedings of the 11th International Conference on
Autonomous Agents and Multiagent Systems-Volume 2,
pp. 837â€“846. International Foundation for Autonomous
Agents and Multiagent Systems, 2012.
Johanson, M., Burch, N., Valenzano, R., and Bowling,
M. Evaluating state-space abstractions in extensive-form
games. InProceedings of the 2013 International Con-
ference on Autonomous Agents and Multiagent Systems,
pp. 271â€“278. International Foundation for Autonomous
Agents and Multiagent Systems, 2013.
Johanson, M. B.Robust Strategies and Counter-Strategies:
From Superhuman to Optimal Play. PhD thesis, Univer-
sity of Alberta, 2016.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization.arXiv preprint arXiv:1412.6980, 2014.

Kroer, C., Farina, G., and Sandholm, T. Solving large
sequential games with the excessive gap technique. In
Advances in Neural Information Processing Systems, pp.
864â€“874, 2018a.

Kroer, C., Waugh, K., KÄ±lÄ±nc Ì§-Karzan, F., and Sandholm, T.
Faster algorithms for extensive-form game solving via
improved smoothing functions.Mathematical Program-
ming, pp. 1â€“33, 2018b.

Lample, G. and Chaplot, D. S. Playing FPS games with
deep reinforcement learning. InAAAI, pp. 2140â€“2146,

Lanctot, M. Monte carlo sampling and regret minimization
for equilibrium computation and decision-making in large
extensive form games. 2013.

Lanctot, M., Waugh, K., Zinkevich, M., and Bowling, M.
Monte Carlo sampling for regret minimization in exten-
sive games. InProceedings of the Annual Conference
on Neural Information Processing Systems (NIPS), pp.
1078â€“1086, 2009.

Li, H., Hu, K., Ge, Z., Jiang, T., Qi, Y., and Song, L. Double
neural counterfactual regret minimization.arXiv preprint
arXiv:1812.10607, 2018.

Littlestone, N. and Warmuth, M. K. The weighted majority
algorithm.Information and Computation, 108(2):212â€“
261, 1994.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning.Nature, 518(7540):
529, 2015.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
chronous methods for deep reinforcement learning. In
International conference on machine learning, pp. 1928â€“
1937, 2016.

MoravË‡c ÌÄ±k, M., Schmid, M., Burch, N., Lisy, V., Morrill, D., Ì
Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowl-
ing, M. Deepstack: Expert-level artificial intelligence in
heads-up no-limit poker.Science, 2017. ISSN 0036-8075.
doi: 10.1126/science.aam6960.

Morrill, D. R. Using regret estimation to solve games com-
pactly. Masterâ€™s thesis, University of Alberta, 2016.

Nash, J. Equilibrium points in n-person games.Proceedings
of the National Academy of Sciences, 36:48â€“49, 1950.
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. Automatic differentiation in pytorch. 2017.
Schmid, M., Burch, N., Lanctot, M., Moravcik, M., Kadlec,
R., and Bowling, M. Variance reduction in monte carlo
counterfactual regret minimization (VR-MCCFR) for ex-
tensive form games using baselines. InAAAI Conference
on Artificial Intelligence (AAAI), 2019.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., et al. Mastering the game of go without
human knowledge.Nature, 550(7676):354, 2017.
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,
M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-
pel, T., et al. A general reinforcement learning algorithm
that masters chess, shogi, and go through self-play.Sci-
ence, 362(6419):1140â€“1144, 2018.
Srinivasan, S., Lanctot, M., Zambaldi, V., Perolat, J., Tuyls, Ì
K., Munos, R., and Bowling, M. Actor-critic policy opti-
mization in partially observable multiagent environments.
InAdvances in Neural Information Processing Systems,
pp. 3426â€“3439, 2018.
Steinberger, E. Single deep counterfactual regret minimiza-
tion.arXiv preprint arXiv:1901.07621, 2019.
Tammelin, O., Burch, N., Johanson, M., and Bowling, M.
Solving heads-up limit texas holdâ€™em. InProceedings
of the International Joint Conference on Artificial Intelli-
gence (IJCAI), pp. 645â€“652, 2015.
Vitter, J. S. Random sampling with a reservoir. ACM
Transactions on Mathematical Software (TOMS), 11(1):
37â€“57, 1985.
Waugh, K. Abstraction in large extensive games. Masterâ€™s
thesis, University of Alberta, 2009.
Waugh, K., Morrill, D., Bagnell, D., and Bowling, M. Solv-
ing games with functional regret estimation. InAAAI
Conference on Artificial Intelligence (AAAI), 2015.
Zinkevich, M., Johanson, M., Bowling, M. H., and Pic-
cione, C. Regret minimization in games with incomplete
information. InProceedings of the Annual Conference
on Neural Information Processing Systems (NIPS), pp.
1729â€“1736, 2007.